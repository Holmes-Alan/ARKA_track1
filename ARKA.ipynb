{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Holmes-Alan/ARKA_track1/blob/main/ARKA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps\n",
        "\n",
        "1. prepare data\n",
        "2. define neural network architecture\n",
        "3. define loss function and optimiser\n",
        "4. train the network on training dataset\n",
        "5. test the network on testing dataset"
      ],
      "metadata": {
        "id": "DotSwb8hAHzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prerequisite\n",
        "\n",
        "prepare data files and required modules\n"
      ],
      "metadata": {
        "id": "swipvx3SVdFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change to working directory\n",
        "%cd drive/MyDrive/ARKA_demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m27L3t_hayXa",
        "outputId": "0ee2ffd2-e8a0-48d4-e7ba-014b80ef48dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ARKA_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jM0X6uFeAGDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data files exist\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3uOynzq5axk",
        "outputId": "5cb847ae-1838-4970-b5de-e3d99afd4a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARKA.ipynb     data_load.py    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  testing_gt.pk  tmp.pptx     validation_gt.pk\n",
            "checkpoint.pt  input_list.txt  readme.jpg    testing.pk     training.pk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF6Wba5CTI9C"
      },
      "outputs": [],
      "source": [
        "# load required python modules\n",
        "import pandas as pd  # for processing DataFrame\n",
        "import numpy as np  # for processing array\n",
        "import torch  # deep learning framework\n",
        "from torch import nn  # nerual network module\n",
        "from torch.utils.data import DataLoader  # prepare dataset\n",
        "import matplotlib.pyplot as plt  # ploting\n",
        "import time  # timer\n",
        "\n",
        "# load chemical data, see \"data_load.py\" for details\n",
        "from data_load import ChemicalDataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset\n",
        "\n",
        "Dataset is usuallly divided into 3 subsets: training/validation/testing  \n",
        "- training set: for fitting the model on the data\n",
        "- validation set: unbiased measurement of ongoing training, for early stopping to prevent over-fitting\n",
        "- testing set: measure the performance of the trained model after training\n",
        "\n",
        "PyTorch Dataset class is a wrapper of data samples  \n",
        "PyTorch Dataloader is utility class to get batches from Dataset\n",
        "\n",
        "PyTorch neural networks support mini-batch natively, network input should be batched."
      ],
      "metadata": {
        "id": "8IBriE1SVrDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ChemicalDataloader(\"training.pk\")  # load train dataset\n",
        "val_dataset = ChemicalDataloader(\"validation_gt.pk\")  # load validation dataset\n",
        "test_dataset = ChemicalDataloader(\"testing_gt.pk\")  # load test dataset\n",
        "\n",
        "# initiate a train dataloader, set batch size, shuffle for each epoch\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# initiate a validation dataloader\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "# initiate a test dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# alternative, split the validation with ground truch for both validation and test\n",
        "# val_dataloader = DataLoader(\n",
        "#     torch.utils.data.Subset(val_dataset, range(len(val_dataset)//2)),\n",
        "#     batch_size=64, shuffle=True)\n",
        "# test_dataloader = DataLoader(\n",
        "#     torch.utils.data.Subset(val_dataset, range(len(val_dataset)//2, len(val_dataset))),\n",
        "#     batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "r7BgjHnpVvh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define network architecture\n",
        "\n",
        "1. define the netowrk layers in `__init__()`\n",
        "2. define how data propagate through the layers in `forward()\n",
        "`"
      ],
      "metadata": {
        "id": "0wmUKq_AVjO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "    Multilayer Perceptron\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        \"\"\"define network architecuture\"\"\"\n",
        "        super().__init__()  # call the init of super class, python convention\n",
        "        # Sequential is a contianer that chain its layers in order\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(49, 300),  # input layer to 1st hidden layer\n",
        "            nn.ReLU(),  # activation function\n",
        "            nn.Linear(300, 600),  # 1st hidden layer to 2nd hidden layer\n",
        "            nn.ReLU(),  # activation function\n",
        "            nn.Linear(600, 46*12)  # output layer, flatten the multidimension output\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        outputs = self.layers(x)  # inputs pass all layers and get outputs\n",
        "        outputs = outputs.reshape((-1,46,12))  # reshape to the original output dimension\n",
        "        return outputs\n",
        "\n",
        "# Initialize the MLP model\n",
        "mlp = MLP()"
      ],
      "metadata": {
        "id": "ujAssWdfVJdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loss function and optimiser"
      ],
      "metadata": {
        "id": "Xh5pXU3RBUm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "\n",
        "# loss func by default return the mean loss of each samples\n",
        "loss_function = nn.MSELoss()  # Mean square error\n",
        "\n",
        "# bind the optimiser on the model weights\n",
        "optimiser = torch.optim.Adam(mlp.parameters(), lr=1e-4)  # adam optimiser"
      ],
      "metadata": {
        "id": "SGY_ejlvBQKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "```python\n",
        "for epoch in epochs:\n",
        "    for batch in dataloader:\n",
        "        intput, target = batch\n",
        "\n",
        "        # 1. forward propagation\n",
        "        output = model(input)\n",
        "\n",
        "        optimiser.zero_grad()  # clear last gradient\n",
        "\n",
        "        # 2. comute loss\n",
        "        loss = loss_func(output, target)\n",
        "\n",
        "        # 3. backward propagation\n",
        "        loss.backwards()\n",
        "\n",
        "        # 4. update weights\n",
        "        optimiser.step()\n",
        "```"
      ],
      "metadata": {
        "id": "XNbKlSD7uiDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### speed up with GPU\n",
        "define a device first to set the target running platform\n",
        "\n",
        "`model.to(device)` or `tensor.to(device)` to move model/tensor to target device\n",
        "\n",
        "be aware of their location, make sure model and tensor on the same platform"
      ],
      "metadata": {
        "id": "97viH25hCffJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set device to cpu\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "C0XADtpwDYf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device to cuda GPU if available\n",
        "# if using colab, check colab runtime type\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EXS1xOafCUCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)\n",
        "mlp.to(device)  # move model to target device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9T80HlgFSaV",
        "outputId": "2ebf3eaa-45bb-4772-8228-9b779d9e6a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=49, out_features=300, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=300, out_features=600, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=600, out_features=552, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "5QomhykKEDNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch):\n",
        "    \"\"\"reshape data for network input\n",
        "\n",
        "    data from ChemicalDataloader is a dictionary, two input features are seperate\n",
        "    this function extract data from dict and concatenate input features\n",
        "    \"\"\"\n",
        "    env = batch['environ']  # envoriment features\n",
        "    init = batch['initial_value']  # inital chemical status\n",
        "    targets = batch['output']  # target future chemical status\n",
        "\n",
        "    inputs = torch.cat([env, init], dim=1)  # concatenate features\n",
        "\n",
        "    inputs = inputs.to(device)  # move inputs tensor to device\n",
        "    targets = targets.to(device)  # move targets tensor to device\n",
        "\n",
        "    return inputs, targets  # return feature and ground truth"
      ],
      "metadata": {
        "id": "GcCqaLJjSfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check the shape of tensors\n",
        "sample1 = train_dataset[0]  # first data sample\n",
        "# print the shape of features and ground truth\n",
        "print(sample1['environ'].shape,\n",
        "      sample1['initial_value'].shape,\n",
        "      sample1['output'].shape)\n",
        "batch1 = next(iter(train_dataloader))  # first batch\n",
        "input1, targets1 = prepare_data(batch1)  # get inputs and targets\n",
        "print(input1.shape, targets1.shape)  # print their shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_xzWJDdbEVJ",
        "outputId": "34727b39-257c-41e5-a8fa-6888b17e4654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,) (46,) (46, 12)\n",
            "torch.Size([64, 49]) torch.Size([64, 46, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set fixed random number seed for reproduction\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Early stopping parameter\n",
        "val_patience = 5  # wait how many epochs when validation loss does not decrease\n",
        "val_count = 0  # number of epochs that validation loss does not decrease\n",
        "val_loss_min = np.Inf  # current minimal validation loss\n",
        "early_stopped = False  # whether has reached the best validation loss\n",
        "best_epoch = 0  # which epoch produced the optimal model\n",
        "\n",
        "# for log\n",
        "print_frequency = 100  # log message every x batches\n",
        "\n",
        "train_losses_epoch = []  # average train loss of each epoch\n",
        "val_losses_epoch = []  # average validation loss of each epoch\n",
        "\n",
        "training_start_time = time.time()  # start time of training\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(100): # max epochs\n",
        "\n",
        "    # set model to train mode, PyTorch convention,\n",
        "    # because some layers behave differently during training and inferencing\n",
        "    mlp.train()\n",
        "\n",
        "    running_loss = 0.0  # loss of recent batches\n",
        "    train_loss_epoch = 0.0  # train loss of batches in one epoch\n",
        "    val_loss_epoch = 0.0  # validation loss of batches in one epoch\n",
        "\n",
        "    # Print epoch\n",
        "    print(f'\\nStarting epoch {epoch+1}')\n",
        "\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Get and prepare inputs\n",
        "        inputs, targets = prepare_data(batch)\n",
        "\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # Perform forward pass\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform optimization\n",
        "        optimiser.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()  # accumulate loss of this batch\n",
        "\n",
        "        if i % print_frequency == print_frequency-1:  # triger print\n",
        "            # average loss every `print_frequency`\n",
        "            avg_loss = running_loss / print_frequency\n",
        "            print(f\"Loss after mini-batch {i+1:5d}: {avg_loss:.9f}\")  # print\n",
        "            train_loss_epoch += running_loss\n",
        "            running_loss = 0.0  # reset loss accumulator\n",
        "\n",
        "    train_loss_epoch /= len(train_dataloader)  # average loss of this epoch\n",
        "    train_losses_epoch.append(train_loss_epoch)  # store loss for each epoch\n",
        "    print(f\"Training has been \"\n",
        "          f\"{(time.time()-training_start_time) / 60:.2f} minutes\")\n",
        "\n",
        "    # early stopping\n",
        "\n",
        "    # set model to evaluation model, PyTorch convention,\n",
        "    # because some layers behave differently during training and infernceing\n",
        "    mlp.eval()\n",
        "    # validation loss of one batch\n",
        "    val_loss_epoch = 0\n",
        "    # no_grad(): context manager, turn off gradient when evaluating,\n",
        "    # turn on gradient automatically when leave the `with` block\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            # Get and prepare inputs\n",
        "            inputs, targets = prepare_data(batch)\n",
        "            # predict the output with mlp model\n",
        "            predicts = mlp(inputs)\n",
        "            # compute test loss, convert from tensor to float, then accumulate\n",
        "            val_loss_epoch += loss_function(predicts, targets).item()\n",
        "\n",
        "    val_loss_epoch /= len(val_dataloader)  # average loss of this epoch\n",
        "    val_losses_epoch.append(val_loss_epoch)  # store loss for each epoch\n",
        "    print(f\"Validation loss: {val_loss_epoch}\")\n",
        "\n",
        "    # if the validation loss decrease, model after this epoch is better,\n",
        "    # therefore for store the current model as the 'best' model for now\n",
        "    if val_loss_epoch < val_loss_min:\n",
        "        if not early_stopped:  # stop saving model if should be stopped\n",
        "            torch.save(mlp.state_dict(), \"checkpoint.pt\")  # save model to file\n",
        "            val_loss_min = val_loss_epoch  # update best loss\n",
        "            val_count = 0  # reset counter\n",
        "            best_epoch = epoch  # record which epoch\n",
        "    else:  # otherwise, we wait a few epochs until no longer decrease\n",
        "        val_count += 1  # increase counter\n",
        "        if val_count > val_patience:  # if the validation loss hasn't improved\n",
        "            early_stopped = True  # should stop\n",
        "            print(f\"early stop at epoch {epoch} \\n\"\n",
        "                  f\"best validation loss is {val_loss_min}\")  # print\n",
        "            break  # early stop\n",
        "\n",
        "\n",
        "\n",
        "# Process is complete.\n",
        "print(\"Training process has finished.\"  # log\n",
        "      f\"using {(time.time()-training_start_time) / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBZxwMd8ZOyl",
        "outputId": "f5e9632c-4858-4e39-8ea4-0b347bee721c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting epoch 1\n",
            "Loss after mini-batch   100: 0.052513020\n",
            "Loss after mini-batch   200: 0.002706159\n",
            "Loss after mini-batch   300: 0.002394916\n",
            "Loss after mini-batch   400: 0.001947719\n",
            "Loss after mini-batch   500: 0.001479351\n",
            "Loss after mini-batch   600: 0.001178011\n",
            "Loss after mini-batch   700: 0.001086171\n",
            "Loss after mini-batch   800: 0.000979855\n",
            "Loss after mini-batch   900: 0.000857850\n",
            "Loss after mini-batch  1000: 0.000741549\n",
            "Loss after mini-batch  1100: 0.000636615\n",
            "Loss after mini-batch  1200: 0.000554232\n",
            "Loss after mini-batch  1300: 0.000499381\n",
            "Training has been 0.40 minutes\n",
            "Validation loss: 0.0004228658637929902\n",
            "\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   100: 0.000408015\n",
            "Loss after mini-batch   200: 0.000371383\n",
            "Loss after mini-batch   300: 0.000336041\n",
            "Loss after mini-batch   400: 0.000309718\n",
            "Loss after mini-batch   500: 0.000288515\n",
            "Loss after mini-batch   600: 0.000268041\n",
            "Loss after mini-batch   700: 0.000253070\n",
            "Loss after mini-batch   800: 0.000245295\n",
            "Loss after mini-batch   900: 0.000234763\n",
            "Loss after mini-batch  1000: 0.000221801\n",
            "Loss after mini-batch  1100: 0.000209397\n",
            "Loss after mini-batch  1200: 0.000198411\n",
            "Loss after mini-batch  1300: 0.000193434\n",
            "Training has been 1.01 minutes\n",
            "Validation loss: 0.00017806960689257468\n",
            "\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   100: 0.000173384\n",
            "Loss after mini-batch   200: 0.000164751\n",
            "Loss after mini-batch   300: 0.000162079\n",
            "Loss after mini-batch   400: 0.000156726\n",
            "Loss after mini-batch   500: 0.000155636\n",
            "Loss after mini-batch   600: 0.000144768\n",
            "Loss after mini-batch   700: 0.000149257\n",
            "Loss after mini-batch   800: 0.000143869\n",
            "Loss after mini-batch   900: 0.000141830\n",
            "Loss after mini-batch  1000: 0.000138271\n",
            "Loss after mini-batch  1100: 0.000130870\n",
            "Loss after mini-batch  1200: 0.000131434\n",
            "Loss after mini-batch  1300: 0.000126769\n",
            "Training has been 1.66 minutes\n",
            "Validation loss: 0.0001257627129289847\n",
            "\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   100: 0.000124832\n",
            "Loss after mini-batch   200: 0.000122981\n",
            "Loss after mini-batch   300: 0.000123288\n",
            "Loss after mini-batch   400: 0.000122411\n",
            "Loss after mini-batch   500: 0.000119644\n",
            "Loss after mini-batch   600: 0.000115925\n",
            "Loss after mini-batch   700: 0.000115907\n",
            "Loss after mini-batch   800: 0.000114199\n",
            "Loss after mini-batch   900: 0.000111032\n",
            "Loss after mini-batch  1000: 0.000112339\n",
            "Loss after mini-batch  1100: 0.000110016\n",
            "Loss after mini-batch  1200: 0.000111546\n",
            "Loss after mini-batch  1300: 0.000108847\n",
            "Training has been 2.23 minutes\n",
            "Validation loss: 0.00010943489427687884\n",
            "\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   100: 0.000105757\n",
            "Loss after mini-batch   200: 0.000106402\n",
            "Loss after mini-batch   300: 0.000105540\n",
            "Loss after mini-batch   400: 0.000103755\n",
            "Loss after mini-batch   500: 0.000105137\n",
            "Loss after mini-batch   600: 0.000100845\n",
            "Loss after mini-batch   700: 0.000102755\n",
            "Loss after mini-batch   800: 0.000107157\n",
            "Loss after mini-batch   900: 0.000100933\n",
            "Loss after mini-batch  1000: 0.000098598\n",
            "Loss after mini-batch  1100: 0.000098767\n",
            "Loss after mini-batch  1200: 0.000102434\n",
            "Loss after mini-batch  1300: 0.000103152\n",
            "Training has been 2.85 minutes\n",
            "Validation loss: 9.604093801205445e-05\n",
            "\n",
            "Starting epoch 6\n",
            "Loss after mini-batch   100: 0.000095818\n",
            "Loss after mini-batch   200: 0.000098213\n",
            "Loss after mini-batch   300: 0.000096909\n",
            "Loss after mini-batch   400: 0.000096355\n",
            "Loss after mini-batch   500: 0.000097414\n",
            "Loss after mini-batch   600: 0.000092575\n",
            "Loss after mini-batch   700: 0.000095781\n",
            "Loss after mini-batch   800: 0.000095113\n",
            "Loss after mini-batch   900: 0.000094002\n",
            "Loss after mini-batch  1000: 0.000093648\n",
            "Loss after mini-batch  1100: 0.000091380\n",
            "Loss after mini-batch  1200: 0.000093557\n",
            "Loss after mini-batch  1300: 0.000091957\n",
            "Training has been 3.39 minutes\n",
            "Validation loss: 9.002319605980137e-05\n",
            "\n",
            "Starting epoch 7\n",
            "Loss after mini-batch   100: 0.000089462\n",
            "Loss after mini-batch   200: 0.000090226\n",
            "Loss after mini-batch   300: 0.000088845\n",
            "Loss after mini-batch   400: 0.000087386\n",
            "Loss after mini-batch   500: 0.000087313\n",
            "Loss after mini-batch   600: 0.000087195\n",
            "Loss after mini-batch   700: 0.000086796\n",
            "Loss after mini-batch   800: 0.000088082\n",
            "Loss after mini-batch   900: 0.000083229\n",
            "Loss after mini-batch  1000: 0.000079503\n",
            "Loss after mini-batch  1100: 0.000082541\n",
            "Loss after mini-batch  1200: 0.000083757\n",
            "Loss after mini-batch  1300: 0.000083114\n",
            "Training has been 3.90 minutes\n",
            "Validation loss: 8.026266579390978e-05\n",
            "\n",
            "Starting epoch 8\n",
            "Loss after mini-batch   100: 0.000079210\n",
            "Loss after mini-batch   200: 0.000080313\n",
            "Loss after mini-batch   300: 0.000079572\n",
            "Loss after mini-batch   400: 0.000077858\n",
            "Loss after mini-batch   500: 0.000077756\n",
            "Loss after mini-batch   600: 0.000076155\n",
            "Loss after mini-batch   700: 0.000077874\n",
            "Loss after mini-batch   800: 0.000072646\n",
            "Loss after mini-batch   900: 0.000073407\n",
            "Loss after mini-batch  1000: 0.000074336\n",
            "Loss after mini-batch  1100: 0.000070980\n",
            "Loss after mini-batch  1200: 0.000069007\n",
            "Loss after mini-batch  1300: 0.000070960\n",
            "Training has been 4.41 minutes\n",
            "Validation loss: 6.804680306389433e-05\n",
            "\n",
            "Starting epoch 9\n",
            "Loss after mini-batch   100: 0.000069359\n",
            "Loss after mini-batch   200: 0.000066440\n",
            "Loss after mini-batch   300: 0.000067664\n",
            "Loss after mini-batch   400: 0.000065653\n",
            "Loss after mini-batch   500: 0.000063667\n",
            "Loss after mini-batch   600: 0.000064054\n",
            "Loss after mini-batch   700: 0.000064857\n",
            "Loss after mini-batch   800: 0.000062945\n",
            "Loss after mini-batch   900: 0.000062600\n",
            "Loss after mini-batch  1000: 0.000060486\n",
            "Loss after mini-batch  1100: 0.000059943\n",
            "Loss after mini-batch  1200: 0.000059396\n",
            "Loss after mini-batch  1300: 0.000057505\n",
            "Training has been 4.92 minutes\n",
            "Validation loss: 6.091110371373929e-05\n",
            "\n",
            "Starting epoch 10\n",
            "Loss after mini-batch   100: 0.000056350\n",
            "Loss after mini-batch   200: 0.000056155\n",
            "Loss after mini-batch   300: 0.000056780\n",
            "Loss after mini-batch   400: 0.000055779\n",
            "Loss after mini-batch   500: 0.000051627\n",
            "Loss after mini-batch   600: 0.000053105\n",
            "Loss after mini-batch   700: 0.000052042\n",
            "Loss after mini-batch   800: 0.000050259\n",
            "Loss after mini-batch   900: 0.000053543\n",
            "Loss after mini-batch  1000: 0.000048179\n",
            "Loss after mini-batch  1100: 0.000047649\n",
            "Loss after mini-batch  1200: 0.000046493\n",
            "Loss after mini-batch  1300: 0.000045818\n",
            "Training has been 5.41 minutes\n",
            "Validation loss: 4.4404111843361216e-05\n",
            "\n",
            "Starting epoch 11\n",
            "Loss after mini-batch   100: 0.000044845\n",
            "Loss after mini-batch   200: 0.000044354\n",
            "Loss after mini-batch   300: 0.000047774\n",
            "Loss after mini-batch   400: 0.000041620\n",
            "Loss after mini-batch   500: 0.000043360\n",
            "Loss after mini-batch   600: 0.000042799\n",
            "Loss after mini-batch   700: 0.000043589\n",
            "Loss after mini-batch   800: 0.000041808\n",
            "Loss after mini-batch   900: 0.000041397\n",
            "Loss after mini-batch  1000: 0.000041515\n",
            "Loss after mini-batch  1100: 0.000040383\n",
            "Loss after mini-batch  1200: 0.000038106\n",
            "Loss after mini-batch  1300: 0.000040053\n",
            "Training has been 5.90 minutes\n",
            "Validation loss: 4.147264801654303e-05\n",
            "\n",
            "Starting epoch 12\n",
            "Loss after mini-batch   100: 0.000039879\n",
            "Loss after mini-batch   200: 0.000037642\n",
            "Loss after mini-batch   300: 0.000037139\n",
            "Loss after mini-batch   400: 0.000037196\n",
            "Loss after mini-batch   500: 0.000034767\n",
            "Loss after mini-batch   600: 0.000037231\n",
            "Loss after mini-batch   700: 0.000036194\n",
            "Loss after mini-batch   800: 0.000033934\n",
            "Loss after mini-batch   900: 0.000035166\n",
            "Loss after mini-batch  1000: 0.000033656\n",
            "Loss after mini-batch  1100: 0.000035732\n",
            "Loss after mini-batch  1200: 0.000033370\n",
            "Loss after mini-batch  1300: 0.000033156\n",
            "Training has been 6.39 minutes\n",
            "Validation loss: 3.3326449720768525e-05\n",
            "\n",
            "Starting epoch 13\n",
            "Loss after mini-batch   100: 0.000034238\n",
            "Loss after mini-batch   200: 0.000032814\n",
            "Loss after mini-batch   300: 0.000034853\n",
            "Loss after mini-batch   400: 0.000032634\n",
            "Loss after mini-batch   500: 0.000031095\n",
            "Loss after mini-batch   600: 0.000031044\n",
            "Loss after mini-batch   700: 0.000031430\n",
            "Loss after mini-batch   800: 0.000031713\n",
            "Loss after mini-batch   900: 0.000031217\n",
            "Loss after mini-batch  1000: 0.000031191\n",
            "Loss after mini-batch  1100: 0.000032469\n",
            "Loss after mini-batch  1200: 0.000033333\n",
            "Loss after mini-batch  1300: 0.000029623\n",
            "Training has been 6.91 minutes\n",
            "Validation loss: 3.3245239719611374e-05\n",
            "\n",
            "Starting epoch 14\n",
            "Loss after mini-batch   100: 0.000030373\n",
            "Loss after mini-batch   200: 0.000029042\n",
            "Loss after mini-batch   300: 0.000029287\n",
            "Loss after mini-batch   400: 0.000030133\n",
            "Loss after mini-batch   500: 0.000031049\n",
            "Loss after mini-batch   600: 0.000028669\n",
            "Loss after mini-batch   700: 0.000029448\n",
            "Loss after mini-batch   800: 0.000027856\n",
            "Loss after mini-batch   900: 0.000030949\n",
            "Loss after mini-batch  1000: 0.000029589\n",
            "Loss after mini-batch  1100: 0.000027449\n",
            "Loss after mini-batch  1200: 0.000027046\n",
            "Loss after mini-batch  1300: 0.000030903\n",
            "Training has been 7.48 minutes\n",
            "Validation loss: 2.7390210336954167e-05\n",
            "\n",
            "Starting epoch 15\n",
            "Loss after mini-batch   100: 0.000028393\n",
            "Loss after mini-batch   200: 0.000026936\n",
            "Loss after mini-batch   300: 0.000028731\n",
            "Loss after mini-batch   400: 0.000028443\n",
            "Loss after mini-batch   500: 0.000028684\n",
            "Loss after mini-batch   600: 0.000027340\n",
            "Loss after mini-batch   700: 0.000029034\n",
            "Loss after mini-batch   800: 0.000029059\n",
            "Loss after mini-batch   900: 0.000027075\n",
            "Loss after mini-batch  1000: 0.000027177\n",
            "Loss after mini-batch  1100: 0.000027824\n",
            "Loss after mini-batch  1200: 0.000028417\n",
            "Loss after mini-batch  1300: 0.000028231\n",
            "Training has been 7.98 minutes\n",
            "Validation loss: 2.5364698864200905e-05\n",
            "\n",
            "Starting epoch 16\n",
            "Loss after mini-batch   100: 0.000027398\n",
            "Loss after mini-batch   200: 0.000026125\n",
            "Loss after mini-batch   300: 0.000027340\n",
            "Loss after mini-batch   400: 0.000026826\n",
            "Loss after mini-batch   500: 0.000027323\n",
            "Loss after mini-batch   600: 0.000027856\n",
            "Loss after mini-batch   700: 0.000025644\n",
            "Loss after mini-batch   800: 0.000027259\n",
            "Loss after mini-batch   900: 0.000026038\n",
            "Loss after mini-batch  1000: 0.000026954\n",
            "Loss after mini-batch  1100: 0.000026066\n",
            "Loss after mini-batch  1200: 0.000026667\n",
            "Loss after mini-batch  1300: 0.000025988\n",
            "Training has been 8.48 minutes\n",
            "Validation loss: 2.7159925512990328e-05\n",
            "\n",
            "Starting epoch 17\n",
            "Loss after mini-batch   100: 0.000027374\n",
            "Loss after mini-batch   200: 0.000027881\n",
            "Loss after mini-batch   300: 0.000025765\n",
            "Loss after mini-batch   400: 0.000025628\n",
            "Loss after mini-batch   500: 0.000026957\n",
            "Loss after mini-batch   600: 0.000026127\n",
            "Loss after mini-batch   700: 0.000025976\n",
            "Loss after mini-batch   800: 0.000027214\n",
            "Loss after mini-batch   900: 0.000023983\n",
            "Loss after mini-batch  1000: 0.000026487\n",
            "Loss after mini-batch  1100: 0.000025744\n",
            "Loss after mini-batch  1200: 0.000023849\n",
            "Loss after mini-batch  1300: 0.000025197\n",
            "Training has been 9.01 minutes\n",
            "Validation loss: 2.5742758791747885e-05\n",
            "\n",
            "Starting epoch 18\n",
            "Loss after mini-batch   100: 0.000026710\n",
            "Loss after mini-batch   200: 0.000025753\n",
            "Loss after mini-batch   300: 0.000026372\n",
            "Loss after mini-batch   400: 0.000026373\n",
            "Loss after mini-batch   500: 0.000026111\n",
            "Loss after mini-batch   600: 0.000025773\n",
            "Loss after mini-batch   700: 0.000025629\n",
            "Loss after mini-batch   800: 0.000024772\n",
            "Loss after mini-batch   900: 0.000026303\n",
            "Loss after mini-batch  1000: 0.000023480\n",
            "Loss after mini-batch  1100: 0.000024311\n",
            "Loss after mini-batch  1200: 0.000024770\n",
            "Loss after mini-batch  1300: 0.000025928\n",
            "Training has been 9.54 minutes\n",
            "Validation loss: 2.7053953111918572e-05\n",
            "\n",
            "Starting epoch 19\n",
            "Loss after mini-batch   100: 0.000023889\n",
            "Loss after mini-batch   200: 0.000026080\n",
            "Loss after mini-batch   300: 0.000025961\n",
            "Loss after mini-batch   400: 0.000024403\n",
            "Loss after mini-batch   500: 0.000025715\n",
            "Loss after mini-batch   600: 0.000025008\n",
            "Loss after mini-batch   700: 0.000024086\n",
            "Loss after mini-batch   800: 0.000026519\n",
            "Loss after mini-batch   900: 0.000024430\n",
            "Loss after mini-batch  1000: 0.000025560\n",
            "Loss after mini-batch  1100: 0.000024487\n",
            "Loss after mini-batch  1200: 0.000024979\n",
            "Loss after mini-batch  1300: 0.000024371\n",
            "Training has been 10.06 minutes\n",
            "Validation loss: 2.312866727822918e-05\n",
            "\n",
            "Starting epoch 20\n",
            "Loss after mini-batch   100: 0.000024988\n",
            "Loss after mini-batch   200: 0.000026103\n",
            "Loss after mini-batch   300: 0.000024854\n",
            "Loss after mini-batch   400: 0.000026045\n",
            "Loss after mini-batch   500: 0.000024842\n",
            "Loss after mini-batch   600: 0.000023168\n",
            "Loss after mini-batch   700: 0.000024912\n",
            "Loss after mini-batch   800: 0.000022887\n",
            "Loss after mini-batch   900: 0.000023859\n",
            "Loss after mini-batch  1000: 0.000023967\n",
            "Loss after mini-batch  1100: 0.000022880\n",
            "Loss after mini-batch  1200: 0.000024481\n",
            "Loss after mini-batch  1300: 0.000023463\n",
            "Training has been 10.60 minutes\n",
            "Validation loss: 2.450424516237055e-05\n",
            "\n",
            "Starting epoch 21\n",
            "Loss after mini-batch   100: 0.000023841\n",
            "Loss after mini-batch   200: 0.000022235\n",
            "Loss after mini-batch   300: 0.000023147\n",
            "Loss after mini-batch   400: 0.000025589\n",
            "Loss after mini-batch   500: 0.000024776\n",
            "Loss after mini-batch   600: 0.000024854\n",
            "Loss after mini-batch   700: 0.000023882\n",
            "Loss after mini-batch   800: 0.000023754\n",
            "Loss after mini-batch   900: 0.000022845\n",
            "Loss after mini-batch  1000: 0.000025148\n",
            "Loss after mini-batch  1100: 0.000023346\n",
            "Loss after mini-batch  1200: 0.000021358\n",
            "Loss after mini-batch  1300: 0.000023079\n",
            "Training has been 11.15 minutes\n",
            "Validation loss: 2.339758330790001e-05\n",
            "\n",
            "Starting epoch 22\n",
            "Loss after mini-batch   100: 0.000023514\n",
            "Loss after mini-batch   200: 0.000023232\n",
            "Loss after mini-batch   300: 0.000024787\n",
            "Loss after mini-batch   400: 0.000022671\n",
            "Loss after mini-batch   500: 0.000023054\n",
            "Loss after mini-batch   600: 0.000021515\n",
            "Loss after mini-batch   700: 0.000021660\n",
            "Loss after mini-batch   800: 0.000022894\n",
            "Loss after mini-batch   900: 0.000023231\n",
            "Loss after mini-batch  1000: 0.000025683\n",
            "Loss after mini-batch  1100: 0.000022205\n",
            "Loss after mini-batch  1200: 0.000023790\n",
            "Loss after mini-batch  1300: 0.000023589\n",
            "Training has been 11.72 minutes\n",
            "Validation loss: 2.1861254097612898e-05\n",
            "\n",
            "Starting epoch 23\n",
            "Loss after mini-batch   100: 0.000021859\n",
            "Loss after mini-batch   200: 0.000021686\n",
            "Loss after mini-batch   300: 0.000022070\n",
            "Loss after mini-batch   400: 0.000022841\n",
            "Loss after mini-batch   500: 0.000021536\n",
            "Loss after mini-batch   600: 0.000022632\n",
            "Loss after mini-batch   700: 0.000023009\n",
            "Loss after mini-batch   800: 0.000023478\n",
            "Loss after mini-batch   900: 0.000023547\n",
            "Loss after mini-batch  1000: 0.000023770\n",
            "Loss after mini-batch  1100: 0.000023016\n",
            "Loss after mini-batch  1200: 0.000022258\n",
            "Loss after mini-batch  1300: 0.000022701\n",
            "Training has been 12.26 minutes\n",
            "Validation loss: 2.2647256405326237e-05\n",
            "\n",
            "Starting epoch 24\n",
            "Loss after mini-batch   100: 0.000023587\n",
            "Loss after mini-batch   200: 0.000022067\n",
            "Loss after mini-batch   300: 0.000023187\n",
            "Loss after mini-batch   400: 0.000022043\n",
            "Loss after mini-batch   500: 0.000021373\n",
            "Loss after mini-batch   600: 0.000022696\n",
            "Loss after mini-batch   700: 0.000021557\n",
            "Loss after mini-batch   800: 0.000022360\n",
            "Loss after mini-batch   900: 0.000021650\n",
            "Loss after mini-batch  1000: 0.000023564\n",
            "Loss after mini-batch  1100: 0.000020892\n",
            "Loss after mini-batch  1200: 0.000022168\n",
            "Loss after mini-batch  1300: 0.000022112\n",
            "Training has been 12.86 minutes\n",
            "Validation loss: 2.050278937034238e-05\n",
            "\n",
            "Starting epoch 25\n",
            "Loss after mini-batch   100: 0.000022038\n",
            "Loss after mini-batch   200: 0.000021360\n",
            "Loss after mini-batch   300: 0.000020853\n",
            "Loss after mini-batch   400: 0.000021802\n",
            "Loss after mini-batch   500: 0.000021158\n",
            "Loss after mini-batch   600: 0.000022039\n",
            "Loss after mini-batch   700: 0.000023263\n",
            "Loss after mini-batch   800: 0.000021991\n",
            "Loss after mini-batch   900: 0.000021097\n",
            "Loss after mini-batch  1000: 0.000021246\n",
            "Loss after mini-batch  1100: 0.000021442\n",
            "Loss after mini-batch  1200: 0.000020689\n",
            "Loss after mini-batch  1300: 0.000022285\n",
            "Training has been 13.42 minutes\n",
            "Validation loss: 2.0216448880139125e-05\n",
            "\n",
            "Starting epoch 26\n",
            "Loss after mini-batch   100: 0.000021371\n",
            "Loss after mini-batch   200: 0.000020969\n",
            "Loss after mini-batch   300: 0.000022135\n",
            "Loss after mini-batch   400: 0.000020853\n",
            "Loss after mini-batch   500: 0.000021232\n",
            "Loss after mini-batch   600: 0.000020277\n",
            "Loss after mini-batch   700: 0.000019987\n",
            "Loss after mini-batch   800: 0.000022799\n",
            "Loss after mini-batch   900: 0.000021497\n",
            "Loss after mini-batch  1000: 0.000022028\n",
            "Loss after mini-batch  1100: 0.000021663\n",
            "Loss after mini-batch  1200: 0.000021343\n",
            "Loss after mini-batch  1300: 0.000020501\n",
            "Training has been 13.99 minutes\n",
            "Validation loss: 2.3915093343395846e-05\n",
            "\n",
            "Starting epoch 27\n",
            "Loss after mini-batch   100: 0.000021058\n",
            "Loss after mini-batch   200: 0.000019834\n",
            "Loss after mini-batch   300: 0.000020343\n",
            "Loss after mini-batch   400: 0.000020778\n",
            "Loss after mini-batch   500: 0.000022311\n",
            "Loss after mini-batch   600: 0.000021359\n",
            "Loss after mini-batch   700: 0.000020509\n",
            "Loss after mini-batch   800: 0.000019352\n",
            "Loss after mini-batch   900: 0.000020660\n",
            "Loss after mini-batch  1000: 0.000021222\n",
            "Loss after mini-batch  1100: 0.000020409\n",
            "Loss after mini-batch  1200: 0.000019555\n",
            "Loss after mini-batch  1300: 0.000019501\n",
            "Training has been 14.59 minutes\n",
            "Validation loss: 2.2156337927338777e-05\n",
            "\n",
            "Starting epoch 28\n",
            "Loss after mini-batch   100: 0.000019818\n",
            "Loss after mini-batch   200: 0.000021111\n",
            "Loss after mini-batch   300: 0.000019764\n",
            "Loss after mini-batch   400: 0.000020323\n",
            "Loss after mini-batch   500: 0.000020565\n",
            "Loss after mini-batch   600: 0.000019984\n",
            "Loss after mini-batch   700: 0.000021071\n",
            "Loss after mini-batch   800: 0.000020588\n",
            "Loss after mini-batch   900: 0.000018591\n",
            "Loss after mini-batch  1000: 0.000020290\n",
            "Loss after mini-batch  1100: 0.000021120\n",
            "Loss after mini-batch  1200: 0.000019429\n",
            "Loss after mini-batch  1300: 0.000020181\n",
            "Training has been 15.19 minutes\n",
            "Validation loss: 2.3291955625048966e-05\n",
            "\n",
            "Starting epoch 29\n",
            "Loss after mini-batch   100: 0.000020231\n",
            "Loss after mini-batch   200: 0.000019391\n",
            "Loss after mini-batch   300: 0.000019593\n",
            "Loss after mini-batch   400: 0.000020399\n",
            "Loss after mini-batch   500: 0.000019290\n",
            "Loss after mini-batch   600: 0.000021410\n",
            "Loss after mini-batch   700: 0.000020527\n",
            "Loss after mini-batch   800: 0.000018715\n",
            "Loss after mini-batch   900: 0.000022434\n",
            "Loss after mini-batch  1000: 0.000020287\n",
            "Loss after mini-batch  1100: 0.000019584\n",
            "Loss after mini-batch  1200: 0.000018979\n",
            "Loss after mini-batch  1300: 0.000019047\n",
            "Training has been 15.76 minutes\n",
            "Validation loss: 1.9426263066464754e-05\n",
            "\n",
            "Starting epoch 30\n",
            "Loss after mini-batch   100: 0.000019082\n",
            "Loss after mini-batch   200: 0.000018860\n",
            "Loss after mini-batch   300: 0.000018850\n",
            "Loss after mini-batch   400: 0.000019114\n",
            "Loss after mini-batch   500: 0.000017815\n",
            "Loss after mini-batch   600: 0.000019350\n",
            "Loss after mini-batch   700: 0.000020995\n",
            "Loss after mini-batch   800: 0.000018010\n",
            "Loss after mini-batch   900: 0.000019356\n",
            "Loss after mini-batch  1000: 0.000019512\n",
            "Loss after mini-batch  1100: 0.000019970\n",
            "Loss after mini-batch  1200: 0.000020249\n",
            "Loss after mini-batch  1300: 0.000019602\n",
            "Training has been 16.36 minutes\n",
            "Validation loss: 1.895334758825659e-05\n",
            "\n",
            "Starting epoch 31\n",
            "Loss after mini-batch   100: 0.000019805\n",
            "Loss after mini-batch   200: 0.000019358\n",
            "Loss after mini-batch   300: 0.000019451\n",
            "Loss after mini-batch   400: 0.000018290\n",
            "Loss after mini-batch   500: 0.000017180\n",
            "Loss after mini-batch   600: 0.000019509\n",
            "Loss after mini-batch   700: 0.000018294\n",
            "Loss after mini-batch   800: 0.000018893\n",
            "Loss after mini-batch   900: 0.000018602\n",
            "Loss after mini-batch  1000: 0.000018495\n",
            "Loss after mini-batch  1100: 0.000020152\n",
            "Loss after mini-batch  1200: 0.000018324\n",
            "Loss after mini-batch  1300: 0.000018393\n",
            "Training has been 16.98 minutes\n",
            "Validation loss: 2.4202395966539786e-05\n",
            "\n",
            "Starting epoch 32\n",
            "Loss after mini-batch   100: 0.000019755\n",
            "Loss after mini-batch   200: 0.000019396\n",
            "Loss after mini-batch   300: 0.000019238\n",
            "Loss after mini-batch   400: 0.000018404\n",
            "Loss after mini-batch   500: 0.000018281\n",
            "Loss after mini-batch   600: 0.000018760\n",
            "Loss after mini-batch   700: 0.000020951\n",
            "Loss after mini-batch   800: 0.000018838\n",
            "Loss after mini-batch   900: 0.000018481\n",
            "Loss after mini-batch  1000: 0.000019051\n",
            "Loss after mini-batch  1100: 0.000018576\n",
            "Loss after mini-batch  1200: 0.000018543\n",
            "Loss after mini-batch  1300: 0.000018467\n",
            "Training has been 17.61 minutes\n",
            "Validation loss: 2.1151905834191623e-05\n",
            "\n",
            "Starting epoch 33\n",
            "Loss after mini-batch   100: 0.000017694\n",
            "Loss after mini-batch   200: 0.000019181\n",
            "Loss after mini-batch   300: 0.000017874\n",
            "Loss after mini-batch   400: 0.000017536\n",
            "Loss after mini-batch   500: 0.000016973\n",
            "Loss after mini-batch   600: 0.000017659\n",
            "Loss after mini-batch   700: 0.000018957\n",
            "Loss after mini-batch   800: 0.000020531\n",
            "Loss after mini-batch   900: 0.000017941\n",
            "Loss after mini-batch  1000: 0.000018401\n",
            "Loss after mini-batch  1100: 0.000018649\n",
            "Loss after mini-batch  1200: 0.000018158\n",
            "Loss after mini-batch  1300: 0.000017768\n",
            "Training has been 18.22 minutes\n",
            "Validation loss: 1.788017612426714e-05\n",
            "\n",
            "Starting epoch 34\n",
            "Loss after mini-batch   100: 0.000018687\n",
            "Loss after mini-batch   200: 0.000016909\n",
            "Loss after mini-batch   300: 0.000018154\n",
            "Loss after mini-batch   400: 0.000019723\n",
            "Loss after mini-batch   500: 0.000017635\n",
            "Loss after mini-batch   600: 0.000017146\n",
            "Loss after mini-batch   700: 0.000018158\n",
            "Loss after mini-batch   800: 0.000016986\n",
            "Loss after mini-batch   900: 0.000019119\n",
            "Loss after mini-batch  1000: 0.000017333\n",
            "Loss after mini-batch  1100: 0.000018406\n",
            "Loss after mini-batch  1200: 0.000017263\n",
            "Loss after mini-batch  1300: 0.000017891\n",
            "Training has been 18.88 minutes\n",
            "Validation loss: 1.7309122900659098e-05\n",
            "\n",
            "Starting epoch 35\n",
            "Loss after mini-batch   100: 0.000017877\n",
            "Loss after mini-batch   200: 0.000017345\n",
            "Loss after mini-batch   300: 0.000017883\n",
            "Loss after mini-batch   400: 0.000018129\n",
            "Loss after mini-batch   500: 0.000016991\n",
            "Loss after mini-batch   600: 0.000017451\n",
            "Loss after mini-batch   700: 0.000017494\n",
            "Loss after mini-batch   800: 0.000018086\n",
            "Loss after mini-batch   900: 0.000018165\n",
            "Loss after mini-batch  1000: 0.000017856\n",
            "Loss after mini-batch  1100: 0.000018082\n",
            "Loss after mini-batch  1200: 0.000018643\n",
            "Loss after mini-batch  1300: 0.000017451\n",
            "Training has been 19.53 minutes\n",
            "Validation loss: 1.6462776966955752e-05\n",
            "\n",
            "Starting epoch 36\n",
            "Loss after mini-batch   100: 0.000017904\n",
            "Loss after mini-batch   200: 0.000017711\n",
            "Loss after mini-batch   300: 0.000017976\n",
            "Loss after mini-batch   400: 0.000018155\n",
            "Loss after mini-batch   500: 0.000016271\n",
            "Loss after mini-batch   600: 0.000017721\n",
            "Loss after mini-batch   700: 0.000016734\n",
            "Loss after mini-batch   800: 0.000017913\n",
            "Loss after mini-batch   900: 0.000017301\n",
            "Loss after mini-batch  1000: 0.000018294\n",
            "Loss after mini-batch  1100: 0.000016851\n",
            "Loss after mini-batch  1200: 0.000016770\n",
            "Loss after mini-batch  1300: 0.000015672\n",
            "Training has been 20.15 minutes\n",
            "Validation loss: 1.7664943445509137e-05\n",
            "\n",
            "Starting epoch 37\n",
            "Loss after mini-batch   100: 0.000018355\n",
            "Loss after mini-batch   200: 0.000016190\n",
            "Loss after mini-batch   300: 0.000018533\n",
            "Loss after mini-batch   400: 0.000018391\n",
            "Loss after mini-batch   500: 0.000017822\n",
            "Loss after mini-batch   600: 0.000016745\n",
            "Loss after mini-batch   700: 0.000016726\n",
            "Loss after mini-batch   800: 0.000017323\n",
            "Loss after mini-batch   900: 0.000016706\n",
            "Loss after mini-batch  1000: 0.000016628\n",
            "Loss after mini-batch  1100: 0.000016520\n",
            "Loss after mini-batch  1200: 0.000017270\n",
            "Loss after mini-batch  1300: 0.000018167\n",
            "Training has been 20.80 minutes\n",
            "Validation loss: 1.6321353807908642e-05\n",
            "\n",
            "Starting epoch 38\n",
            "Loss after mini-batch   100: 0.000016588\n",
            "Loss after mini-batch   200: 0.000016666\n",
            "Loss after mini-batch   300: 0.000016496\n",
            "Loss after mini-batch   400: 0.000016953\n",
            "Loss after mini-batch   500: 0.000015188\n",
            "Loss after mini-batch   600: 0.000017834\n",
            "Loss after mini-batch   700: 0.000016853\n",
            "Loss after mini-batch   800: 0.000016041\n",
            "Loss after mini-batch   900: 0.000017142\n",
            "Loss after mini-batch  1000: 0.000016507\n",
            "Loss after mini-batch  1100: 0.000015484\n",
            "Loss after mini-batch  1200: 0.000016640\n",
            "Loss after mini-batch  1300: 0.000018196\n",
            "Training has been 21.44 minutes\n",
            "Validation loss: 1.4522010414212335e-05\n",
            "\n",
            "Starting epoch 39\n",
            "Loss after mini-batch   100: 0.000015631\n",
            "Loss after mini-batch   200: 0.000015401\n",
            "Loss after mini-batch   300: 0.000017215\n",
            "Loss after mini-batch   400: 0.000019606\n",
            "Loss after mini-batch   500: 0.000015734\n",
            "Loss after mini-batch   600: 0.000016142\n",
            "Loss after mini-batch   700: 0.000015816\n",
            "Loss after mini-batch   800: 0.000019013\n",
            "Loss after mini-batch   900: 0.000016700\n",
            "Loss after mini-batch  1000: 0.000016853\n",
            "Loss after mini-batch  1100: 0.000015779\n",
            "Loss after mini-batch  1200: 0.000015998\n",
            "Loss after mini-batch  1300: 0.000016790\n",
            "Training has been 22.10 minutes\n",
            "Validation loss: 1.5048571963914923e-05\n",
            "\n",
            "Starting epoch 40\n",
            "Loss after mini-batch   100: 0.000017546\n",
            "Loss after mini-batch   200: 0.000016121\n",
            "Loss after mini-batch   300: 0.000017625\n",
            "Loss after mini-batch   400: 0.000015951\n",
            "Loss after mini-batch   500: 0.000017266\n",
            "Loss after mini-batch   600: 0.000015991\n",
            "Loss after mini-batch   700: 0.000016330\n",
            "Loss after mini-batch   800: 0.000015938\n",
            "Loss after mini-batch   900: 0.000015301\n",
            "Loss after mini-batch  1000: 0.000015908\n",
            "Loss after mini-batch  1100: 0.000015416\n",
            "Loss after mini-batch  1200: 0.000015022\n",
            "Loss after mini-batch  1300: 0.000015552\n",
            "Training has been 22.77 minutes\n",
            "Validation loss: 1.413710048132631e-05\n",
            "\n",
            "Starting epoch 41\n",
            "Loss after mini-batch   100: 0.000015007\n",
            "Loss after mini-batch   200: 0.000016610\n",
            "Loss after mini-batch   300: 0.000016795\n",
            "Loss after mini-batch   400: 0.000016326\n",
            "Loss after mini-batch   500: 0.000017377\n",
            "Loss after mini-batch   600: 0.000015991\n",
            "Loss after mini-batch   700: 0.000016544\n",
            "Loss after mini-batch   800: 0.000016701\n",
            "Loss after mini-batch   900: 0.000014731\n",
            "Loss after mini-batch  1000: 0.000014768\n",
            "Loss after mini-batch  1100: 0.000015982\n",
            "Loss after mini-batch  1200: 0.000015798\n",
            "Loss after mini-batch  1300: 0.000015468\n",
            "Training has been 23.44 minutes\n",
            "Validation loss: 1.4452479882609607e-05\n",
            "\n",
            "Starting epoch 42\n",
            "Loss after mini-batch   100: 0.000015203\n",
            "Loss after mini-batch   200: 0.000015000\n",
            "Loss after mini-batch   300: 0.000015395\n",
            "Loss after mini-batch   400: 0.000015946\n",
            "Loss after mini-batch   500: 0.000014967\n",
            "Loss after mini-batch   600: 0.000017116\n",
            "Loss after mini-batch   700: 0.000017172\n",
            "Loss after mini-batch   800: 0.000015858\n",
            "Loss after mini-batch   900: 0.000014338\n",
            "Loss after mini-batch  1000: 0.000016557\n",
            "Loss after mini-batch  1100: 0.000015347\n",
            "Loss after mini-batch  1200: 0.000014473\n",
            "Loss after mini-batch  1300: 0.000016474\n",
            "Training has been 24.10 minutes\n",
            "Validation loss: 1.6887539940657156e-05\n",
            "\n",
            "Starting epoch 43\n",
            "Loss after mini-batch   100: 0.000015895\n",
            "Loss after mini-batch   200: 0.000015388\n",
            "Loss after mini-batch   300: 0.000014832\n",
            "Loss after mini-batch   400: 0.000014643\n",
            "Loss after mini-batch   500: 0.000015599\n",
            "Loss after mini-batch   600: 0.000017715\n",
            "Loss after mini-batch   700: 0.000017672\n",
            "Loss after mini-batch   800: 0.000016812\n",
            "Loss after mini-batch   900: 0.000016251\n",
            "Loss after mini-batch  1000: 0.000014408\n",
            "Loss after mini-batch  1100: 0.000016013\n",
            "Loss after mini-batch  1200: 0.000015202\n",
            "Loss after mini-batch  1300: 0.000014793\n",
            "Training has been 24.77 minutes\n",
            "Validation loss: 1.3241806731429852e-05\n",
            "\n",
            "Starting epoch 44\n",
            "Loss after mini-batch   100: 0.000014627\n",
            "Loss after mini-batch   200: 0.000015282\n",
            "Loss after mini-batch   300: 0.000013414\n",
            "Loss after mini-batch   400: 0.000016412\n",
            "Loss after mini-batch   500: 0.000016514\n",
            "Loss after mini-batch   600: 0.000015069\n",
            "Loss after mini-batch   700: 0.000016408\n",
            "Loss after mini-batch   800: 0.000013990\n",
            "Loss after mini-batch   900: 0.000016303\n",
            "Loss after mini-batch  1000: 0.000014811\n",
            "Loss after mini-batch  1100: 0.000015226\n",
            "Loss after mini-batch  1200: 0.000014651\n",
            "Loss after mini-batch  1300: 0.000015047\n",
            "Training has been 25.42 minutes\n",
            "Validation loss: 1.463557031725041e-05\n",
            "\n",
            "Starting epoch 45\n",
            "Loss after mini-batch   100: 0.000015301\n",
            "Loss after mini-batch   200: 0.000013538\n",
            "Loss after mini-batch   300: 0.000015500\n",
            "Loss after mini-batch   400: 0.000015411\n",
            "Loss after mini-batch   500: 0.000013764\n",
            "Loss after mini-batch   600: 0.000015059\n",
            "Loss after mini-batch   700: 0.000015094\n",
            "Loss after mini-batch   800: 0.000014189\n",
            "Loss after mini-batch   900: 0.000016160\n",
            "Loss after mini-batch  1000: 0.000014925\n",
            "Loss after mini-batch  1100: 0.000014677\n",
            "Loss after mini-batch  1200: 0.000015662\n",
            "Loss after mini-batch  1300: 0.000014805\n",
            "Training has been 26.08 minutes\n",
            "Validation loss: 1.3990193824838078e-05\n",
            "\n",
            "Starting epoch 46\n",
            "Loss after mini-batch   100: 0.000014852\n",
            "Loss after mini-batch   200: 0.000014267\n",
            "Loss after mini-batch   300: 0.000013958\n",
            "Loss after mini-batch   400: 0.000014672\n",
            "Loss after mini-batch   500: 0.000013448\n",
            "Loss after mini-batch   600: 0.000015300\n",
            "Loss after mini-batch   700: 0.000014474\n",
            "Loss after mini-batch   800: 0.000014870\n",
            "Loss after mini-batch   900: 0.000015522\n",
            "Loss after mini-batch  1000: 0.000014757\n",
            "Loss after mini-batch  1100: 0.000014088\n",
            "Loss after mini-batch  1200: 0.000014583\n",
            "Loss after mini-batch  1300: 0.000014423\n",
            "Training has been 26.77 minutes\n",
            "Validation loss: 1.7686129816818665e-05\n",
            "\n",
            "Starting epoch 47\n",
            "Loss after mini-batch   100: 0.000014087\n",
            "Loss after mini-batch   200: 0.000014461\n",
            "Loss after mini-batch   300: 0.000015684\n",
            "Loss after mini-batch   400: 0.000014247\n",
            "Loss after mini-batch   500: 0.000014270\n",
            "Loss after mini-batch   600: 0.000014498\n",
            "Loss after mini-batch   700: 0.000014335\n",
            "Loss after mini-batch   800: 0.000014589\n",
            "Loss after mini-batch   900: 0.000014348\n",
            "Loss after mini-batch  1000: 0.000014300\n",
            "Loss after mini-batch  1100: 0.000013096\n",
            "Loss after mini-batch  1200: 0.000013388\n",
            "Loss after mini-batch  1300: 0.000015127\n",
            "Training has been 27.44 minutes\n",
            "Validation loss: 1.5114669630453854e-05\n",
            "\n",
            "Starting epoch 48\n",
            "Loss after mini-batch   100: 0.000015059\n",
            "Loss after mini-batch   200: 0.000014120\n",
            "Loss after mini-batch   300: 0.000013149\n",
            "Loss after mini-batch   400: 0.000014918\n",
            "Loss after mini-batch   500: 0.000014423\n",
            "Loss after mini-batch   600: 0.000013239\n",
            "Loss after mini-batch   700: 0.000015675\n",
            "Loss after mini-batch   800: 0.000013657\n",
            "Loss after mini-batch   900: 0.000013819\n",
            "Loss after mini-batch  1000: 0.000013501\n",
            "Loss after mini-batch  1100: 0.000016402\n",
            "Loss after mini-batch  1200: 0.000015099\n",
            "Loss after mini-batch  1300: 0.000015675\n",
            "Training has been 28.10 minutes\n",
            "Validation loss: 1.4765459131320672e-05\n",
            "\n",
            "Starting epoch 49\n",
            "Loss after mini-batch   100: 0.000014631\n",
            "Loss after mini-batch   200: 0.000013429\n",
            "Loss after mini-batch   300: 0.000014974\n",
            "Loss after mini-batch   400: 0.000014911\n",
            "Loss after mini-batch   500: 0.000014717\n",
            "Loss after mini-batch   600: 0.000013550\n",
            "Loss after mini-batch   700: 0.000014366\n",
            "Loss after mini-batch   800: 0.000013457\n",
            "Loss after mini-batch   900: 0.000013876\n",
            "Loss after mini-batch  1000: 0.000015086\n",
            "Loss after mini-batch  1100: 0.000013797\n",
            "Loss after mini-batch  1200: 0.000012966\n",
            "Loss after mini-batch  1300: 0.000013234\n",
            "Training has been 28.78 minutes\n",
            "Validation loss: 1.3233112488838138e-05\n",
            "\n",
            "Starting epoch 50\n",
            "Loss after mini-batch   100: 0.000014038\n",
            "Loss after mini-batch   200: 0.000013444\n",
            "Loss after mini-batch   300: 0.000012928\n",
            "Loss after mini-batch   400: 0.000014082\n",
            "Loss after mini-batch   500: 0.000013846\n",
            "Loss after mini-batch   600: 0.000013131\n",
            "Loss after mini-batch   700: 0.000014534\n",
            "Loss after mini-batch   800: 0.000014016\n",
            "Loss after mini-batch   900: 0.000013996\n",
            "Loss after mini-batch  1000: 0.000012930\n",
            "Loss after mini-batch  1100: 0.000014175\n",
            "Loss after mini-batch  1200: 0.000015031\n",
            "Loss after mini-batch  1300: 0.000013468\n",
            "Training has been 29.43 minutes\n",
            "Validation loss: 1.1845628786374703e-05\n",
            "\n",
            "Starting epoch 51\n",
            "Loss after mini-batch   100: 0.000012766\n",
            "Loss after mini-batch   200: 0.000012943\n",
            "Loss after mini-batch   300: 0.000014517\n",
            "Loss after mini-batch   400: 0.000013144\n",
            "Loss after mini-batch   500: 0.000015166\n",
            "Loss after mini-batch   600: 0.000013176\n",
            "Loss after mini-batch   700: 0.000012920\n",
            "Loss after mini-batch   800: 0.000013318\n",
            "Loss after mini-batch   900: 0.000013353\n",
            "Loss after mini-batch  1000: 0.000013789\n",
            "Loss after mini-batch  1100: 0.000013429\n",
            "Loss after mini-batch  1200: 0.000012678\n",
            "Loss after mini-batch  1300: 0.000014191\n",
            "Training has been 30.09 minutes\n",
            "Validation loss: 1.368069246315142e-05\n",
            "\n",
            "Starting epoch 52\n",
            "Loss after mini-batch   100: 0.000012936\n",
            "Loss after mini-batch   200: 0.000013749\n",
            "Loss after mini-batch   300: 0.000012211\n",
            "Loss after mini-batch   400: 0.000014063\n",
            "Loss after mini-batch   500: 0.000012356\n",
            "Loss after mini-batch   600: 0.000014247\n",
            "Loss after mini-batch   700: 0.000012703\n",
            "Loss after mini-batch   800: 0.000013290\n",
            "Loss after mini-batch   900: 0.000012727\n",
            "Loss after mini-batch  1000: 0.000013574\n",
            "Loss after mini-batch  1100: 0.000012946\n",
            "Loss after mini-batch  1200: 0.000014046\n",
            "Loss after mini-batch  1300: 0.000013811\n",
            "Training has been 30.76 minutes\n",
            "Validation loss: 1.2733098765929042e-05\n",
            "\n",
            "Starting epoch 53\n",
            "Loss after mini-batch   100: 0.000013094\n",
            "Loss after mini-batch   200: 0.000014535\n",
            "Loss after mini-batch   300: 0.000013111\n",
            "Loss after mini-batch   400: 0.000013197\n",
            "Loss after mini-batch   500: 0.000012828\n",
            "Loss after mini-batch   600: 0.000012284\n",
            "Loss after mini-batch   700: 0.000014504\n",
            "Loss after mini-batch   800: 0.000012705\n",
            "Loss after mini-batch   900: 0.000014300\n",
            "Loss after mini-batch  1000: 0.000012406\n",
            "Loss after mini-batch  1100: 0.000012633\n",
            "Loss after mini-batch  1200: 0.000012762\n",
            "Loss after mini-batch  1300: 0.000012448\n",
            "Training has been 31.44 minutes\n",
            "Validation loss: 1.3690866540429355e-05\n",
            "\n",
            "Starting epoch 54\n",
            "Loss after mini-batch   100: 0.000012466\n",
            "Loss after mini-batch   200: 0.000013359\n",
            "Loss after mini-batch   300: 0.000013184\n",
            "Loss after mini-batch   400: 0.000012960\n",
            "Loss after mini-batch   500: 0.000012297\n",
            "Loss after mini-batch   600: 0.000013170\n",
            "Loss after mini-batch   700: 0.000012283\n",
            "Loss after mini-batch   800: 0.000013023\n",
            "Loss after mini-batch   900: 0.000011889\n",
            "Loss after mini-batch  1000: 0.000014745\n",
            "Loss after mini-batch  1100: 0.000012816\n",
            "Loss after mini-batch  1200: 0.000012812\n",
            "Loss after mini-batch  1300: 0.000011980\n",
            "Training has been 32.09 minutes\n",
            "Validation loss: 1.542861916401907e-05\n",
            "\n",
            "Starting epoch 55\n",
            "Loss after mini-batch   100: 0.000012786\n",
            "Loss after mini-batch   200: 0.000012265\n",
            "Loss after mini-batch   300: 0.000012954\n",
            "Loss after mini-batch   400: 0.000013212\n",
            "Loss after mini-batch   500: 0.000012228\n",
            "Loss after mini-batch   600: 0.000011532\n",
            "Loss after mini-batch   700: 0.000012473\n",
            "Loss after mini-batch   800: 0.000011853\n",
            "Loss after mini-batch   900: 0.000013354\n",
            "Loss after mini-batch  1000: 0.000012787\n",
            "Loss after mini-batch  1100: 0.000012274\n",
            "Loss after mini-batch  1200: 0.000013192\n",
            "Loss after mini-batch  1300: 0.000011225\n",
            "Training has been 32.76 minutes\n",
            "Validation loss: 1.3310285601090667e-05\n",
            "\n",
            "Starting epoch 56\n",
            "Loss after mini-batch   100: 0.000012503\n",
            "Loss after mini-batch   200: 0.000011956\n",
            "Loss after mini-batch   300: 0.000013788\n",
            "Loss after mini-batch   400: 0.000012393\n",
            "Loss after mini-batch   500: 0.000012053\n",
            "Loss after mini-batch   600: 0.000012443\n",
            "Loss after mini-batch   700: 0.000011348\n",
            "Loss after mini-batch   800: 0.000012943\n",
            "Loss after mini-batch   900: 0.000012236\n",
            "Loss after mini-batch  1000: 0.000012966\n",
            "Loss after mini-batch  1100: 0.000012082\n",
            "Loss after mini-batch  1200: 0.000011820\n",
            "Loss after mini-batch  1300: 0.000011555\n",
            "Training has been 33.42 minutes\n",
            "Validation loss: 1.4234977136641956e-05\n",
            "early stop at epoch 55best validation loss is 1.1845628786374703e-05\n",
            "Training process has finished.using 33.48 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loss\n",
        "# skip the first epoch since its loss is much larger\n",
        "# and make it hard to scale the plot\n",
        "\n",
        "# plot train loss per epoch\n",
        "plt.plot(range(2,len(train_losses_epoch)+1), train_losses_epoch[1:], label=\"train\")\n",
        "# plot validation loss per epoch\n",
        "plt.plot(range(2,len(train_losses_epoch)+1), val_losses_epoch[1:], label=\"validation\")\n",
        "plt.legend()  # show the legends\n",
        "plt.show()  # show plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "tJSzt5UxmWKY",
        "outputId": "67de138f-f281-4a28-d95f-13ee68d0f062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "0d75M3fbBaY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test on testing dataset\n",
        "\n",
        "best_model = MLP()\n",
        "best_model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "best_model.to(device)\n",
        "\n",
        "test_loss = 0  # sum of each testing batch\n",
        "\n",
        "# set model to evaluation mode, PyTorch convention,\n",
        "# because some layers behave differently during training and infernceing\n",
        "best_model.eval()\n",
        "\n",
        "# no_grad(): context manager, turn off gradient when evaluating,\n",
        "# turn on gradient automatically when leave the `with` block\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(test_dataloader):  # for each test batch\n",
        "        # Get and prepare inputs\n",
        "        inputs, targets = prepare_data(batch)\n",
        "        # predict the output with mlp model\n",
        "        predicts = best_model(inputs)\n",
        "        # compute test loss, convert from tensor to float, then accumulate\n",
        "        test_loss += loss_function(predicts, targets).item()\n",
        "    # average loss of all batches\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "print(f\"Loss on test dataset is {test_loss}\")  # print the testing loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGRVvj8sE2u-",
        "outputId": "084eff54-fe0d-433a-8e5f-ebdeb8d668f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on test dataset is 1.1991272164586542e-05\n"
          ]
        }
      ]
    }
  ]
}